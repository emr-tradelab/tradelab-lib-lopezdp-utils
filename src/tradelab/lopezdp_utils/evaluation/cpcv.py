"""Combinatorial Purged Cross-Validation (CPCV) — AFML Chapter 12.

References:
    López de Prado, "Advances in Financial Machine Learning", Chapter 12
"""

from itertools import combinations
from math import comb

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold

from tradelab.lopezdp_utils.modeling.cross_validation import (
    get_embargo_times,
    get_train_times,
)


def get_num_splits(n_groups: int, k_test_groups: int) -> int:
    """Compute number of train/test splits in CPCV.

    Args:
        n_groups: Total number of groups (N).
        k_test_groups: Number of test groups per split (k).

    Returns:
        C(N, k).
    """
    return comb(n_groups, k_test_groups)


def get_num_backtest_paths(n_groups: int, k_test_groups: int) -> int:
    """Compute number of complete backtest paths generated by CPCV.

    Formula: phi[N,k] = k/N * C(N,k)

    Args:
        n_groups: Total number of groups (N).
        k_test_groups: Number of test groups per split (k).

    Returns:
        Number of complete backtest paths.
    """
    return k_test_groups * comb(n_groups, k_test_groups) // n_groups


class CombinatorialPurgedKFold(KFold):
    """Combinatorial Purged Cross-Validation splitter.

    Extends KFold to generate all combinatorial train/test splits
    with purging and embargoing.

    Args:
        n_splits: Number of groups (N). Default 6.
        k_test_groups: Number of test groups per split (k). Default 2.
        t1: Series of label through-dates (index=start, value=end).
        pct_embargo: Fraction of observations to embargo after test sets.
    """

    def __init__(
        self,
        n_splits: int = 6,
        k_test_groups: int = 2,
        t1: pd.Series | None = None,
        pct_embargo: float = 0.0,
    ):
        if not isinstance(t1, pd.Series):
            raise ValueError("Label Through Dates must be a pd.Series")
        if k_test_groups >= n_splits:
            raise ValueError("k_test_groups must be < n_splits")
        super().__init__(n_splits=n_splits, shuffle=False, random_state=None)
        self.k_test_groups = k_test_groups
        self.t1 = t1
        self.pct_embargo = pct_embargo

    def split(self, X, y=None, groups=None):
        """Generate all combinatorial purged train/test splits.

        Yields:
            Tuple of (train_indices, test_indices).
        """
        if (X.index == self.t1.index).sum() != len(self.t1):
            raise ValueError("X and ThruDateValues must have the same index")

        indices = np.arange(X.shape[0])
        group_boundaries = np.array_split(indices, self.n_splits)

        for test_group_indices in combinations(range(self.n_splits), self.k_test_groups):
            test_indices = np.concatenate([group_boundaries[g] for g in test_group_indices])

            test_times = self.t1.iloc[test_indices]
            train_t1 = get_train_times(self.t1, test_times)
            train_indices = self.t1.index.searchsorted(train_t1.index)

            if self.pct_embargo > 0:
                mbrg = get_embargo_times(self.t1.index, self.pct_embargo)
                for g in test_group_indices:
                    test_end_idx = group_boundaries[g][-1]
                    if test_end_idx < X.shape[0] - 1:
                        embargo_end = mbrg.iloc[test_end_idx]
                        embargo_mask = self.t1.index.searchsorted(embargo_end)
                        embargo_zone = indices[test_end_idx + 1 : embargo_mask]
                        train_indices = np.setdiff1d(train_indices, embargo_zone)

            yield train_indices, test_indices

    def get_test_group_map(self, X):
        """Return mapping of (test_group_ids) -> test_indices for each split."""
        indices = np.arange(X.shape[0])
        group_boundaries = np.array_split(indices, self.n_splits)

        result = []
        for test_group_indices in combinations(range(self.n_splits), self.k_test_groups):
            test_indices = np.concatenate([group_boundaries[g] for g in test_group_indices])
            result.append((test_group_indices, test_indices))
        return result


def assemble_backtest_paths(
    predictions: dict[int, pd.DataFrame],
    n_groups: int,
    k_test_groups: int,
    n_obs: int,
) -> list[np.ndarray]:
    """Assemble OOS forecasts from CPCV splits into complete backtest paths.

    Args:
        predictions: Dict mapping split_index -> OOS predictions DataFrame.
        n_groups: Total number of groups (N).
        k_test_groups: Number of test groups per split (k).
        n_obs: Total number of observations (T).

    Returns:
        List of numpy arrays, one per backtest path.
    """
    all_groups = set(range(n_groups))
    all_splits = list(combinations(range(n_groups), k_test_groups))

    # Build mapping from split_index to group tuple
    split_to_groups = {}
    for i, split_groups in enumerate(all_splits):
        split_to_groups[i] = split_groups

    obs_per_group = np.array_split(np.arange(n_obs), n_groups)
    paths = []

    def _find_partitions(remaining, available, current):
        if not remaining:
            paths.append(list(current))
            return
        for i, split in enumerate(available):
            split_set = set(split)
            if split_set.issubset(remaining):
                _find_partitions(remaining - split_set, available[i + 1 :], [*current, split])

    phi = get_num_backtest_paths(n_groups, k_test_groups)
    remainder = n_groups % k_test_groups
    if remainder == 0:
        _find_partitions(all_groups, all_splits, [])

        result = []
        for partition in paths[:phi]:
            path_preds = np.empty(n_obs)
            path_preds[:] = np.nan
            for split_groups in partition:
                split_idx = all_splits.index(split_groups)
                if split_idx in predictions:
                    preds = predictions[split_idx]
                    group_obs = np.concatenate([obs_per_group[g] for g in split_groups])
                    if hasattr(preds, "values"):
                        path_preds[group_obs] = preds.values.flatten()[: len(group_obs)]
                    else:
                        path_preds[group_obs] = preds.flatten()[: len(group_obs)]
            result.append(path_preds)
        return result

    phi = get_num_backtest_paths(n_groups, k_test_groups)
    _find_partitions(all_groups, all_splits, [])

    result = []
    for partition in paths[:phi]:
        path_preds = np.empty(n_obs)
        path_preds[:] = np.nan
        for split_groups in partition:
            split_idx = all_splits.index(split_groups)
            if split_idx in predictions:
                preds = predictions[split_idx]
                group_obs = np.concatenate([obs_per_group[g] for g in split_groups])
                if hasattr(preds, "values"):
                    path_preds[group_obs] = preds.values.flatten()[: len(group_obs)]
                else:
                    path_preds[group_obs] = preds.flatten()[: len(group_obs)]
        result.append(path_preds)
    return result

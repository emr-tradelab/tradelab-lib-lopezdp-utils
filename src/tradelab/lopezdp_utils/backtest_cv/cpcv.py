"""Combinatorial Purged Cross-Validation (CPCV).

Implements the CPCV backtesting method from AFML Chapter 12. CPCV overcomes
the "single path" limitation of Walk-Forward and standard CV by generating
multiple out-of-sample backtest paths through combinatorial splits.

The algorithm:
1. Partition T observations into N contiguous groups (no shuffling).
2. Compute all C(N, k) splits where N-k groups train and k groups test.
3. Apply purging and embargoing (via Ch.7 logic) to each split.
4. Fit classifiers and produce OOS forecasts for each split.
5. Assemble φ[N,k] = k/N * C(N,k) complete backtest paths.

Reference:
    López de Prado, M. (2018). *Advances in Financial Machine Learning*. Chapter 12.
"""

from itertools import combinations
from math import comb

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold

from tradelab.lopezdp_utils.cross_validation.purging import (
    get_embargo_times,
    get_train_times,
)


def get_num_splits(n_groups: int, k_test_groups: int) -> int:
    """Compute number of train/test splits in CPCV.

    The total number of unique combinations of N groups taken k at a time
    for testing, with the remaining N-k groups used for training.

    Formula: C(N, k) = N! / (k! * (N-k)!)

    Args:
        n_groups: Total number of groups (N) to partition observations into.
        k_test_groups: Number of groups assigned to the test set in each split (k).

    Returns:
        Number of unique train/test splits.

    Reference:
        AFML Section 12.3.
    """
    return comb(n_groups, k_test_groups)


def get_num_backtest_paths(n_groups: int, k_test_groups: int) -> int:
    """Compute number of complete backtest paths generated by CPCV.

    Each path covers all T observations exactly once. The number of paths
    depends on how many groups are tested per split.

    Formula: φ[N,k] = k/N * C(N,k)

    Special cases:
    - k=1: φ = 1 (reduces to standard CV, single path)
    - k=2: φ = N-1 paths (efficient rule of thumb)

    Args:
        n_groups: Total number of groups (N).
        k_test_groups: Number of test groups per split (k).
            Should satisfy k <= N/2 to ensure >= 50% training data.

    Returns:
        Number of complete backtest paths (φ).

    Reference:
        AFML Section 12.3.
    """
    return k_test_groups * comb(n_groups, k_test_groups) // n_groups


class CombinatorialPurgedKFold(KFold):
    """Combinatorial Purged Cross-Validation splitter.

    Extends scikit-learn's KFold to generate all combinatorial train/test
    splits with purging and embargoing. Unlike standard PurgedKFold (Ch.7)
    which generates N splits with 1 test group each, CPCV generates C(N,k)
    splits with k test groups each.

    This produces φ[N,k] = k/N * C(N,k) complete backtest paths, enabling
    an empirical distribution of strategy performance metrics.

    Key constraints:
    - Shuffle is forced to False (observations must remain in time order).
    - k should be <= N/2 to ensure training set is >= 50% of data.
    - Groups are contiguous blocks of observations (no shuffling).

    Args:
        n_splits: Number of groups to partition data into (N). Default 6.
        k_test_groups: Number of groups per test set (k). Default 2.
        t1: pandas Series of label "through dates" (index = start, value = end).
        pct_embargo: Fraction of observations to embargo after test sets.

    Reference:
        AFML Section 12.3.
    """

    def __init__(
        self,
        n_splits: int = 6,
        k_test_groups: int = 2,
        t1: pd.Series | None = None,
        pct_embargo: float = 0.0,
    ):
        if not isinstance(t1, pd.Series):
            raise ValueError("Label Through Dates must be a pd.Series")
        if k_test_groups >= n_splits:
            raise ValueError("k_test_groups must be < n_splits")
        super().__init__(n_splits=n_splits, shuffle=False, random_state=None)
        self.k_test_groups = k_test_groups
        self.t1 = t1
        self.pct_embargo = pct_embargo

    def split(
        self,
        X: pd.DataFrame,
        y: pd.Series | None = None,
        groups: np.ndarray | None = None,
    ):
        """Generate all combinatorial purged train/test splits.

        Yields C(N, k) train/test index pairs where k groups form the test
        set and the remaining N-k groups form the training set (after purging
        and embargoing).

        Each split also carries metadata about which groups are in the test set,
        enabling downstream path assembly.

        Yields:
            Tuple of (train_indices, test_indices) as numpy arrays.
        """
        if (X.index == self.t1.index).sum() != len(self.t1):
            raise ValueError("X and ThruDateValues must have the same index")

        indices = np.arange(X.shape[0])

        # Step 1: Partition into N contiguous groups
        group_boundaries = np.array_split(indices, self.n_splits)

        # Step 2: Generate all C(N, k) combinations of test groups
        for test_group_indices in combinations(range(self.n_splits), self.k_test_groups):
            # Collect test indices from selected groups
            test_indices = np.concatenate([group_boundaries[g] for g in test_group_indices])

            # Step 3: Purge and embargo
            # Start with all non-test indices as candidates for training
            test_times = self.t1.iloc[test_indices]
            train_t1 = get_train_times(self.t1, test_times)
            train_indices = self.t1.index.searchsorted(train_t1.index)

            # Apply embargo
            if self.pct_embargo > 0:
                mbrg = get_embargo_times(self.t1.index, self.pct_embargo)
                # For each test group, find the end of the embargo period
                # and remove training observations that fall within it
                for g in test_group_indices:
                    test_end_idx = group_boundaries[g][-1]
                    if test_end_idx < X.shape[0] - 1:
                        embargo_end = mbrg.iloc[test_end_idx]
                        embargo_mask = self.t1.index.searchsorted(embargo_end)
                        # Remove training indices in the embargo zone
                        embargo_zone = indices[test_end_idx + 1 : embargo_mask]
                        train_indices = np.setdiff1d(train_indices, embargo_zone)

            yield train_indices, test_indices

    def get_test_group_map(
        self,
        X: pd.DataFrame,
    ) -> list[tuple[tuple[int, ...], np.ndarray]]:
        """Return a mapping of (test_group_ids) -> test_indices for each split.

        Useful for assembling backtest paths: each split's test groups need to
        be distributed across the φ paths.

        Args:
            X: Feature matrix (used only for index alignment).

        Returns:
            List of (group_tuple, test_indices) for each combinatorial split.
        """
        indices = np.arange(X.shape[0])
        group_boundaries = np.array_split(indices, self.n_splits)

        result = []
        for test_group_indices in combinations(range(self.n_splits), self.k_test_groups):
            test_indices = np.concatenate([group_boundaries[g] for g in test_group_indices])
            result.append((test_group_indices, test_indices))
        return result


def assemble_backtest_paths(
    predictions: dict[tuple[int, ...], np.ndarray],
    n_groups: int,
    k_test_groups: int,
    n_obs: int,
) -> list[np.ndarray]:
    """Assemble OOS forecasts from CPCV splits into complete backtest paths.

    Each path covers all T observations exactly once. Given C(N,k) splits
    where each split tests k groups, this function distributes forecasts
    across φ[N,k] paths such that each group appears exactly once per path.

    The assembly works by finding sets of splits whose test groups partition
    all N groups. Each such partition defines one complete backtest path.

    Args:
        predictions: Dict mapping (test_group_ids) -> OOS predictions array.
            Keys are tuples of group indices that were tested in that split.
            Values are prediction arrays covering those test groups' observations.
        n_groups: Total number of groups (N).
        k_test_groups: Number of test groups per split (k).
        n_obs: Total number of observations (T).

    Returns:
        List of numpy arrays, one per backtest path. Each array has length T
        containing the OOS predictions for that path.

    Reference:
        AFML Section 12.3, Step 5.
    """
    all_groups = set(range(n_groups))

    # Find all valid partitions: sets of splits whose test groups cover all N groups
    all_splits = list(combinations(range(n_groups), k_test_groups))

    remainder = n_groups % k_test_groups

    paths = []

    def _find_partitions(
        remaining_groups: set[int],
        available_splits: list[tuple[int, ...]],
        current_partition: list[tuple[int, ...]],
        depth: int,
    ):
        """Recursively find all sets of non-overlapping splits covering all groups."""
        if not remaining_groups:
            paths.append(list(current_partition))
            return

        for i, split in enumerate(available_splits):
            split_set = set(split)
            if split_set.issubset(remaining_groups):
                _find_partitions(
                    remaining_groups - split_set,
                    available_splits[i + 1 :],
                    [*current_partition, split],
                    depth + 1,
                )

    # Only find exact partitions when N is divisible by k
    if remainder == 0:
        _find_partitions(all_groups, all_splits, [], 0)

        # Assemble predictions for each path
        result = []
        # Build a mapping from group index to observation indices
        obs_per_group = np.array_split(np.arange(n_obs), n_groups)

        for partition in paths:
            path_predictions = np.empty(n_obs)
            path_predictions[:] = np.nan
            for split_groups in partition:
                preds = predictions[split_groups]
                # Map predictions to their observation positions
                group_obs = np.concatenate([obs_per_group[g] for g in split_groups])
                path_predictions[group_obs] = preds
            result.append(path_predictions)

        return result

    # When N is not divisible by k, use greedy approach to maximize coverage
    # Each group appears in C(N-1, k-1) splits. Assign each group's prediction
    # from any split that tested it.
    # This is the general case: average predictions across all splits that test each group
    obs_per_group = np.array_split(np.arange(n_obs), n_groups)
    phi = get_num_backtest_paths(n_groups, k_test_groups)

    result = []
    used_partitions = set()

    def _find_partial_partitions(
        remaining_groups: set[int],
        available_splits: list[tuple[int, ...]],
        current_partition: list[tuple[int, ...]],
    ):
        """Find partitions covering as many groups as possible."""
        if not remaining_groups:
            key = tuple(sorted(tuple(sorted(s)) for s in current_partition))
            if key not in used_partitions:
                used_partitions.add(key)
                paths.append(list(current_partition))
            return

        found = False
        for i, split in enumerate(available_splits):
            split_set = set(split)
            if split_set.issubset(remaining_groups):
                found = True
                _find_partial_partitions(
                    remaining_groups - split_set,
                    available_splits[i + 1 :],
                    [*current_partition, split],
                )

        if not found and current_partition:
            key = tuple(sorted(tuple(sorted(s)) for s in current_partition))
            if key not in used_partitions:
                used_partitions.add(key)
                paths.append(list(current_partition))

    _find_partial_partitions(all_groups, all_splits, [])

    for partition in paths[:phi]:
        path_predictions = np.empty(n_obs)
        path_predictions[:] = np.nan
        for split_groups in partition:
            preds = predictions[split_groups]
            group_obs = np.concatenate([obs_per_group[g] for g in split_groups])
            path_predictions[group_obs] = preds
        result.append(path_predictions)

    return result
